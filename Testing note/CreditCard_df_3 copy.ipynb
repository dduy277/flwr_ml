{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available. Training will run on CPU.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
    "else:\n",
    "    print(\"No GPU available. Training will run on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zuy/anaconda3/envs/flower_tutorial/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, precision_recall_curve, log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "from flwr_datasets.partitioner import IidPartitioner\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load partition df_train_3 data.\"\"\"\n",
    "df = pd.read_csv('/home/zuy/Documents/BCU/ML/CSV/df_train_3.csv')\n",
    "df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "dataset = Dataset.from_pandas(df)\n",
    "partitioner = IidPartitioner(num_partitions=5)\n",
    "partitioner.dataset = dataset\n",
    "dataset = partitioner.load_partition(partition_id=1).to_pandas()\n",
    "\n",
    "# .values to fix: X has feature names, but LogisticRegression was fitted without feature names\n",
    "# X = dataset.drop('Class', axis=1).values\n",
    "# y = dataset['Class'].values\n",
    "# # Split the on edge data: 80% train, 20% test\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader, testloader= train_test_split(dataset, test_size=0.2, random_state=42, stratify=dataset['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = trainloader.drop('Class', axis=1)\n",
    "y_train = trainloader['Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "skearn \n",
    "minmaxscale ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28536</th>\n",
       "      <td>0.544834</td>\n",
       "      <td>0.921701</td>\n",
       "      <td>-0.060181</td>\n",
       "      <td>-0.823946</td>\n",
       "      <td>0.105318</td>\n",
       "      <td>0.090517</td>\n",
       "      <td>-0.907818</td>\n",
       "      <td>0.301038</td>\n",
       "      <td>-0.810353</td>\n",
       "      <td>0.552154</td>\n",
       "      <td>-0.031500</td>\n",
       "      <td>-0.414985</td>\n",
       "      <td>0.463892</td>\n",
       "      <td>0.138274</td>\n",
       "      <td>0.372314</td>\n",
       "      <td>-0.306193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35730</th>\n",
       "      <td>0.850756</td>\n",
       "      <td>-0.314666</td>\n",
       "      <td>0.584753</td>\n",
       "      <td>-0.707728</td>\n",
       "      <td>0.425283</td>\n",
       "      <td>0.636531</td>\n",
       "      <td>0.812520</td>\n",
       "      <td>1.150059</td>\n",
       "      <td>0.889625</td>\n",
       "      <td>-0.610177</td>\n",
       "      <td>-0.058563</td>\n",
       "      <td>-0.515112</td>\n",
       "      <td>-0.818758</td>\n",
       "      <td>-1.491316</td>\n",
       "      <td>1.387307</td>\n",
       "      <td>2.114823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10522</th>\n",
       "      <td>0.824575</td>\n",
       "      <td>0.843705</td>\n",
       "      <td>0.287426</td>\n",
       "      <td>-0.470351</td>\n",
       "      <td>2.405082</td>\n",
       "      <td>0.525068</td>\n",
       "      <td>0.629222</td>\n",
       "      <td>0.065386</td>\n",
       "      <td>-0.076589</td>\n",
       "      <td>-0.548481</td>\n",
       "      <td>1.451546</td>\n",
       "      <td>-0.963727</td>\n",
       "      <td>-0.004566</td>\n",
       "      <td>0.402805</td>\n",
       "      <td>-0.206822</td>\n",
       "      <td>-0.095616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32937</th>\n",
       "      <td>-0.004663</td>\n",
       "      <td>0.544923</td>\n",
       "      <td>0.077508</td>\n",
       "      <td>-0.045872</td>\n",
       "      <td>0.375620</td>\n",
       "      <td>-0.081098</td>\n",
       "      <td>-0.139800</td>\n",
       "      <td>-0.136544</td>\n",
       "      <td>0.056967</td>\n",
       "      <td>0.290506</td>\n",
       "      <td>-0.185244</td>\n",
       "      <td>-0.282374</td>\n",
       "      <td>-0.821401</td>\n",
       "      <td>-1.072505</td>\n",
       "      <td>0.006372</td>\n",
       "      <td>-0.278636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29177</th>\n",
       "      <td>-0.386347</td>\n",
       "      <td>-0.388697</td>\n",
       "      <td>0.495550</td>\n",
       "      <td>0.493308</td>\n",
       "      <td>-0.460490</td>\n",
       "      <td>0.851869</td>\n",
       "      <td>0.173766</td>\n",
       "      <td>0.423927</td>\n",
       "      <td>0.275991</td>\n",
       "      <td>-0.437238</td>\n",
       "      <td>-0.625745</td>\n",
       "      <td>0.656433</td>\n",
       "      <td>0.317799</td>\n",
       "      <td>0.018283</td>\n",
       "      <td>-0.335834</td>\n",
       "      <td>-0.281280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41411</th>\n",
       "      <td>0.588787</td>\n",
       "      <td>0.888975</td>\n",
       "      <td>-0.144245</td>\n",
       "      <td>-0.624796</td>\n",
       "      <td>0.212572</td>\n",
       "      <td>-0.061745</td>\n",
       "      <td>-0.505496</td>\n",
       "      <td>-0.003170</td>\n",
       "      <td>-0.307216</td>\n",
       "      <td>0.318287</td>\n",
       "      <td>0.344063</td>\n",
       "      <td>0.691793</td>\n",
       "      <td>0.773736</td>\n",
       "      <td>-0.366522</td>\n",
       "      <td>0.570380</td>\n",
       "      <td>-0.293807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39212</th>\n",
       "      <td>-0.123966</td>\n",
       "      <td>0.489708</td>\n",
       "      <td>-0.097760</td>\n",
       "      <td>0.632929</td>\n",
       "      <td>0.812796</td>\n",
       "      <td>-0.760210</td>\n",
       "      <td>0.232086</td>\n",
       "      <td>-0.655078</td>\n",
       "      <td>0.420258</td>\n",
       "      <td>0.585224</td>\n",
       "      <td>0.075915</td>\n",
       "      <td>0.706847</td>\n",
       "      <td>0.882414</td>\n",
       "      <td>-0.334412</td>\n",
       "      <td>-0.141753</td>\n",
       "      <td>-0.306054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40665</th>\n",
       "      <td>-0.074422</td>\n",
       "      <td>-0.242867</td>\n",
       "      <td>0.789619</td>\n",
       "      <td>0.733392</td>\n",
       "      <td>0.576917</td>\n",
       "      <td>-0.182604</td>\n",
       "      <td>-0.536166</td>\n",
       "      <td>0.528554</td>\n",
       "      <td>-0.155711</td>\n",
       "      <td>-0.182666</td>\n",
       "      <td>0.168943</td>\n",
       "      <td>-0.234173</td>\n",
       "      <td>-0.773053</td>\n",
       "      <td>-0.922916</td>\n",
       "      <td>0.463810</td>\n",
       "      <td>-0.216423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11358</th>\n",
       "      <td>-0.621570</td>\n",
       "      <td>0.517404</td>\n",
       "      <td>0.094948</td>\n",
       "      <td>0.103966</td>\n",
       "      <td>0.478090</td>\n",
       "      <td>-0.315753</td>\n",
       "      <td>-0.466010</td>\n",
       "      <td>-0.091485</td>\n",
       "      <td>-0.070143</td>\n",
       "      <td>0.203788</td>\n",
       "      <td>-0.201232</td>\n",
       "      <td>0.171330</td>\n",
       "      <td>-0.394828</td>\n",
       "      <td>-0.862745</td>\n",
       "      <td>-0.032462</td>\n",
       "      <td>-0.181072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18678</th>\n",
       "      <td>-0.475016</td>\n",
       "      <td>-0.355765</td>\n",
       "      <td>0.578639</td>\n",
       "      <td>0.694289</td>\n",
       "      <td>-0.385267</td>\n",
       "      <td>0.235384</td>\n",
       "      <td>-0.178824</td>\n",
       "      <td>0.772640</td>\n",
       "      <td>-0.207906</td>\n",
       "      <td>-0.339821</td>\n",
       "      <td>-0.633583</td>\n",
       "      <td>-0.172486</td>\n",
       "      <td>-0.037309</td>\n",
       "      <td>0.200966</td>\n",
       "      <td>0.009165</td>\n",
       "      <td>0.179541</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36455 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Time        V1        V2        V3        V4        V5        V6  \\\n",
       "28536  0.544834  0.921701 -0.060181 -0.823946  0.105318  0.090517 -0.907818   \n",
       "35730  0.850756 -0.314666  0.584753 -0.707728  0.425283  0.636531  0.812520   \n",
       "10522  0.824575  0.843705  0.287426 -0.470351  2.405082  0.525068  0.629222   \n",
       "32937 -0.004663  0.544923  0.077508 -0.045872  0.375620 -0.081098 -0.139800   \n",
       "29177 -0.386347 -0.388697  0.495550  0.493308 -0.460490  0.851869  0.173766   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "41411  0.588787  0.888975 -0.144245 -0.624796  0.212572 -0.061745 -0.505496   \n",
       "39212 -0.123966  0.489708 -0.097760  0.632929  0.812796 -0.760210  0.232086   \n",
       "40665 -0.074422 -0.242867  0.789619  0.733392  0.576917 -0.182604 -0.536166   \n",
       "11358 -0.621570  0.517404  0.094948  0.103966  0.478090 -0.315753 -0.466010   \n",
       "18678 -0.475016 -0.355765  0.578639  0.694289 -0.385267  0.235384 -0.178824   \n",
       "\n",
       "             V7        V8        V9       V10       V11       V12       V13  \\\n",
       "28536  0.301038 -0.810353  0.552154 -0.031500 -0.414985  0.463892  0.138274   \n",
       "35730  1.150059  0.889625 -0.610177 -0.058563 -0.515112 -0.818758 -1.491316   \n",
       "10522  0.065386 -0.076589 -0.548481  1.451546 -0.963727 -0.004566  0.402805   \n",
       "32937 -0.136544  0.056967  0.290506 -0.185244 -0.282374 -0.821401 -1.072505   \n",
       "29177  0.423927  0.275991 -0.437238 -0.625745  0.656433  0.317799  0.018283   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "41411 -0.003170 -0.307216  0.318287  0.344063  0.691793  0.773736 -0.366522   \n",
       "39212 -0.655078  0.420258  0.585224  0.075915  0.706847  0.882414 -0.334412   \n",
       "40665  0.528554 -0.155711 -0.182666  0.168943 -0.234173 -0.773053 -0.922916   \n",
       "11358 -0.091485 -0.070143  0.203788 -0.201232  0.171330 -0.394828 -0.862745   \n",
       "18678  0.772640 -0.207906 -0.339821 -0.633583 -0.172486 -0.037309  0.200966   \n",
       "\n",
       "            V14    Amount  \n",
       "28536  0.372314 -0.306193  \n",
       "35730  1.387307  2.114823  \n",
       "10522 -0.206822 -0.095616  \n",
       "32937  0.006372 -0.278636  \n",
       "29177 -0.335834 -0.281280  \n",
       "...         ...       ...  \n",
       "41411  0.570380 -0.293807  \n",
       "39212 -0.141753 -0.306054  \n",
       "40665  0.463810 -0.216423  \n",
       "11358 -0.032462 -0.181072  \n",
       "18678  0.009165  0.179541  \n",
       "\n",
       "[36455 rows x 16 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36455, 16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_test = torch.from_numpy(np.expand_dims(X_train, axis =1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([36455, 1, 16])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_test2 = DataLoader(X_train, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_26622/2151899429.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train_test2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# X_shape = X_train_test2.dataset.data.shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/flower_tutorial/lib/python3.11/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6295\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6296\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6297\u001b[0m         ):\n\u001b[1;32m   6298\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6299\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "data = X_train_test2.dataset.data \n",
    "# X_shape = X_train_test2.dataset.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = LogisticRegression(max_iter=50, penalty=\"l2\", warm_start=False)\n",
    "# get initial globlal parameters\n",
    "\n",
    "n_classes = 2  # Number of class in dataset (y) \n",
    "n_features = 16  # Number of features in dataset\n",
    "lg.classes_ = np.array([i for i in range(n_classes)])\n",
    "\n",
    "lg.coef_ = np.zeros((n_classes, n_features))\n",
    "if lg.fit_intercept:\n",
    "    lg.intercept_ = np.zeros((n_classes,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.coef_ = params[0]\n",
    "# if model.fit_intercept:\n",
    "#     model.intercept_ = params[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Not Fraud       1.00      1.00      1.00      9102\n",
      "       Fraud       0.75      0.75      0.75        12\n",
      "\n",
      "    accuracy                           1.00      9114\n",
      "   macro avg       0.87      0.87      0.87      9114\n",
      "weighted avg       1.00      1.00      1.00      9114\n",
      "\n",
      "ROC_AUC: 0.9997619570790303\n",
      "AUC: 0.8505723905723906\n",
      "Log_loss: 0.0020138044875679696\n"
     ]
    }
   ],
   "source": [
    "lg_model = lg.fit(X_train, y_train)\n",
    "fpr, tpr, _ = roc_curve(y_test, lg_model.predict_proba(X_test)[:, 1])\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, lg_model.predict_proba(X_test)[:, 1])\n",
    "# plot_roc(fpr, tpr)\n",
    "# plot_prc(precision, recall)\n",
    "print(classification_report(y_test, lg_model.predict(X_test), target_names=['Not Fraud', 'Fraud']))\n",
    "print(\"ROC_AUC:\", roc_auc_score(y_test, lg_model.predict_proba(X_test)[:, 1]))\n",
    "print(\"AUC:\", auc(recall, precision))\n",
    "print(\"Log_loss:\", log_loss(y_test,lg_model.predict_proba(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.RNN(10, 20, 2)\n",
    "input = torch.randn(5, 3, 10)\n",
    "h0 = torch.randn(2, 3, 20)\n",
    "output, h1 = rnn(input, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output2, h2 = rnn(input, h1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flower-3.11.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
